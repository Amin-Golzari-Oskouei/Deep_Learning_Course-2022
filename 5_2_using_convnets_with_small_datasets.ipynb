{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "V1eCSWlyLwEA",
        "outputId": "e7a9b899-1732-497f-f5f2-9b03cd99e581"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import keras\n",
        "keras.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"right\">\n",
        "<font size=\"+2\" face=\"homa\">\n",
        "  <b>\n",
        " شبکه کانولوشنی روی مجموعه داده کوچک\n",
        "  <br><br>\n",
        "  <font size=\"+1\" face=\"homa\">\n",
        "  فصل ۵ قسمت ۲ \n",
        "  </b>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "  <b>\n",
        "آموزش شبکه کانولوشنی از ابتدا روی مجموعه داده کوچک\n",
        "  </b>\n",
        "<br>\n",
        "آموزش مدل دسته‌بندی تصویر با استفاده از نمونه‌های بسیار کم، چالشی است که اگر به صورت حرفه‌ای بینایی ماشین کار کرده باشید، در عمل با آن مواجه شده‌اید. تعداد کمی نمونه، می‌تواند چند صد تا چند ده هزار تصویر را شامل شود. به عنوان یک مثال عملی، روی دسته‌بندی تصاویر به عنوان سگ‌ یا گربه در مجموعه ¬داده¬ای با 4 هزار تصویر گربه و سگ (هر کدام 2 هزار) تمرکز می‌کنیم. از 2000 تصویر برای آموزش، 1000 تصویر را برای اعتبارسنجی و 1000 تصویر را برای آزمایش استفاده خواهیم کرد.\n",
        "<br>\n",
        "در این بخش، برای مدیریت این مسئله یکی از راهبردهای پایه را مرور خواهیم کرد که عبارت است از آموزش مدل جدید از ابتدا با استفاده از نمونه‌های اندک موجود. با آموزش ابتدایی شبکه کانولوشنی روی 2000 نمونه آموزشی و بدون هیچ گونه تنظیم، کار را شروع خواهیم کرد تا برای آنچه می‌توان به دست آورد یک حد پایین  تعریف کرده باشیم. با این کار به دقت دسته‌بندی 71 درصد خواهیم رسید. در این نقطه، مشکل اصلی بیش‌برازش خواهد بود. سپس داده¬افزایی  را معرفی خواهیم کرد که راهبرد نیرومندی برای کاهش بیش‌برازش در بینایی ماشین است. با استفاده از داده¬افزایی، شبکه بهبود داده می‌شود و دقت آن به 82 درصد خواهد رسید.\n",
        "<br>\n",
        "در این بخش، دو راهکار ضروری برای اعمال یادگیری عمیق در مجموعه ¬داده‌های کوچک را مرور خواهیم کرد: استخراج ویژگی با شبکه از پیش آموزش دیده  (که دقت را به 90 درصد تا 96 درصد خواهد رساند) و تنظیم دقیق  شبکه از پیش آموزش دیده (که دقت نهایی 97 درصد را رقم خواهد زد). این سه راهکار یعنی آموزش مدل کوچک از ابتدا، استخراج ویژگی با استفاده از مدل از پیش آموزش دیده و تنظیم دقیق مدل از پیش آموزش دیده، جعبه ابزار ما برای مدیریت مسئله دسته‌بندی تصویر با استفاده از مجموعه داده‌های کوچک را تشکیل خواهند داد.\n",
        "\n"
      ],
      "metadata": {
        "id": "NP19AMc4L1EA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"right\">\n",
        "<font size=\"+2\" face=\"homa\">\n",
        "  <b>\n",
        " ارتباط یادگیری عمیق با مسائل مجموعه داده کوچک\n",
        "  <br><br>\n",
        "  <font size=\"+1\" face=\"homa\">\n",
        "  </b>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "<br>\n",
        "ممکن است شنیده باشید که یادگیری عمیق فقط زمانی کاربرد دارد که نمونه‌های زیادی در دسترس باشند. این گفته تا حدودی معتبر است. یکی از مشخصه‌های عمده یادگیری عمیق این است که قابلیت پیدا کردن برخی ویژگی‌های جالب در نمونه‌های آموزشی بدون نیاز به مهندسی دستی ویژگی را دارد و این تنها زمانی پژوهشگر می‌شود که مثال‌های آموزشی زیادی در دسترس باشند. این امر به ویژه در مورد مسائلی حقیقت دارد که در آن نمونه‌های ورودی دارای ابعاد بالا هستند، مانند تصاویر.\n",
        "<br>\n",
        "اما تعریف نمونه‌های بسیار نسبی است (به عنوان اولین نکته، به اندازه و عمق شبکه‌ای که می‌خواهید آموزش دهید بستگی دارد). با چند ده نمونه نمی‌توان یک شبکه کانولوشنی را برای حل مسائل پیچیده آموزش داد، اما در صورتی که مدل کوچک باشد و به خوبی تنظیم شده باشد و مسئله نیز ساده باشد، چند صد نمونه کفایت خواهد کرد. از آنجایی که شبکه‌های کانولوشنی ویژگی‌های مستقل از مکان و محلی را یاد می‌گیرند، در مسائل ادراکی، از نظر نیاز به نمونه بسیار کارآمد هستند. آموزش شبکه کانولوشنی از ابتدا روی مجموعه داده کوچکی از تصاویر، علی‌رغم کمبود نسبی نمونه‌ها، بدون نیاز به هرگونه مهندسی ویژگی، نتایج معقولی تولید خواهد کرد. در این قسمت این مسئله را به صورت عملی مشاهده خواهید کرد.\n",
        "<br>\n",
        "به علاوه، مدل‌های یادگیری عمیق به صورت ذاتی دارای قابلیت هدف‌گذاری مجدد هستند: مثلاً می‌توانید مدل دسته‌بندی تصویر یا تبدیل متن به گفتار را که روی یک مجموعه¬ داده بزرگ آموزش داده‌اید با تغییرات جزئی در مسئله کاملاً متفاوتی مجدداً مورد استفاده قرار دهید. به ویژه در مورد بینایی ماشین، مدل‌های از پیش آموزش دیده بسیاری (اغلب با استفاده از مجموعه داده ایمج‌نت آموزش دیده‌اند) برای دانلود برای عموم قابل دسترس هستند و می‌توان از آن‌ها برای راه اندازی مدل‌های بینایی قوی با نمونه‌های بسیار اندک استفاده کرد. همان کاری که در قسمت بعدی انجام خواهیم داد. کارمان را با بارگیری نمونه‌ها شروع می‌کنیم."
      ],
      "metadata": {
        "id": "RXuEy8MjN4nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"right\">\n",
        "<font size=\"+2\" face=\"homa\">\n",
        "  <b>\n",
        " دانلود داده ها\n",
        "  <br><br>\n",
        "  <font size=\"+1\" face=\"homa\">\n",
        "  </b>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "<br>\n",
        "مجموعه داده سگ‌ها- درمقابل-گربه‌ها که از آن استفاده خواهید کرد در پکیج کراس موجود نیست. کگل آن را به عنوان بخشی از رقابت بینایی ماشین در اواخر 2013، یعنی زمانی که شبکه‌های کانولوشنی شایع نبودند، در اختیار عموم قرار داد. می‌توانید مجموعه داده اصلی را از سایت کگل دانلود کنید (اگر در کگل حساب ندارید باید ابتدا یک حساب باز کنید، اما نگران نباشید فرایند آن ساده است).\n",
        "<br>\n",
        "تصاویر این مجموعه رنگی بوده و وضوح متوسطی دارند. \n",
        "<br>"
      ],
      "metadata": {
        "id": "GQkR5UvCN4iI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cats_vs_dogs_samples](https://s3.amazonaws.com/book.keras.io/img/ch5/cats_vs_dogs_samples.jpg)"
      ],
      "metadata": {
        "id": "8l0Rcdh7OqpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "اینکه برنده رقابت 2013 سگ‌ها-در مقابل-گربه‌های کگل شرکت‌کننده‌هایی بودند که از شبکه کانولوشنی استفاده کرده بودند، غافلگیر کننده نبود. بهترین دقت تا 95 درصد بود. در این مثال، کاملاً به این دقت نزدیک خواهید شد (در بخش بعدی)، هر چند که نمونه‌های مدل‌هایی که آموزش خواهید داد کمتر از 10 درصد نمونه‌هایی است که در اختیار رقبایتان قرار داشت.\n",
        "این مجموعه داده شامل 25 هزار تصویر سگ و گربه (12 هزار و پانصد برای هر کلاس) و 543 مگابایت (فشرده) است. بعد از دانلود و باز کردن، یک مجموعه داده جدید با سه زیرمجموعه خواهیم ساخت: مجموعه آموزشی با 1000 نمونه از هر کلاس، اعتبارسنجی با 250 نمونه از هر کلاس و مجموعه آزمایش با 250 نمونه از هر کلاس.\n",
        "کد این مجموعه‌ها در زیر آمده است.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "75SrdzWmOwVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "!pip install unzip\n",
        "!unzip cats_and_dogs_filtered"
      ],
      "metadata": {
        "id": "hYAzpDl8Wzy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3Nu59GgaLwEN"
      },
      "outputs": [],
      "source": [
        "import os, shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N4LSsE65LwEO"
      },
      "outputs": [],
      "source": [
        "# The path to the directory where the original\n",
        "# dataset was uncompressed\n",
        "original_dataset_dir = '/content/cats_and_dogs_filtered'\n",
        "\n",
        "# The directory where we will\n",
        "# store our smaller dataset\n",
        "base_dir = '/content/cats_and_dogs_small'\n",
        "os.mkdir(base_dir)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "os.mkdir(train_cats_dir)\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "os.mkdir(train_dogs_dir)\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "os.mkdir(validation_cats_dir)\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "os.mkdir(validation_dogs_dir)\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "test_cats_dir = os.path.join(test_dir, 'cats')\n",
        "os.mkdir(test_cats_dir)\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
        "os.mkdir(test_dogs_dir)\n",
        "\n",
        "# Copy first 1000 cat images to train_cats_dir\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join('/content/cats_and_dogs_filtered/train/cats', fname)\n",
        "    dst = os.path.join(train_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "# Copy next 250 cat images to validation_cats_dir\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(2000, 2250)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join('/content/cats_and_dogs_filtered/validation/cats', fname)\n",
        "    dst = os.path.join(validation_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "# Copy next 250 cat images to test_cats_dir\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(2250, 2500)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join('/content/cats_and_dogs_filtered/validation/cats', fname)\n",
        "    dst = os.path.join(test_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "# Copy first 1000 dog images to train_dogs_dir\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join('/content/cats_and_dogs_filtered/train/dogs', fname)\n",
        "    dst = os.path.join(train_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "# Copy next 250 dog images to validation_dogs_dir\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(2000, 2250)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join('/content/cats_and_dogs_filtered/validation/dogs', fname)\n",
        "    dst = os.path.join(validation_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "# Copy next 250 dog images to test_dogs_dir\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(2250, 2500)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join('/content/cats_and_dogs_filtered/validation/dogs', fname)\n",
        "    dst = os.path.join(test_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "برای بررسی صحت اطلاعات، تعداد تصاویر را در هر بخش (آموزش/ اعتبارسنجی/ آزمایش) خواهیم شمرد:\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "5EIu86MAc8Z-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkQiWQOPLwER",
        "outputId": "c0c59812-98cd-473e-97ab-171fbd3b143a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training cat images: 1000\n"
          ]
        }
      ],
      "source": [
        "print('total training cat images:', len(os.listdir(train_cats_dir)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX8m3azaLwET",
        "outputId": "d501c19e-45b5-46d0-cceb-f78c8e764154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training dog images: 1000\n"
          ]
        }
      ],
      "source": [
        "print('total training dog images:', len(os.listdir(train_dogs_dir)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nvoe8o2LwEU",
        "outputId": "60cdeb6f-8292-484a-cdea-c964a6c7bcf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total validation cat images: 250\n"
          ]
        }
      ],
      "source": [
        "print('total validation cat images:', len(os.listdir(validation_cats_dir)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtmxwAHpLwEV",
        "outputId": "3726cf86-b677-4286-8090-50d1a5da09f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total validation dog images: 250\n"
          ]
        }
      ],
      "source": [
        "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urSlPGC_LwEW",
        "outputId": "a4a37a7a-09f8-403f-de40-12f1c8e11600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total test cat images: 250\n"
          ]
        }
      ],
      "source": [
        "print('total test cat images:', len(os.listdir(test_cats_dir)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjfzPk3-LwEY",
        "outputId": "47c57b7a-eda3-4fdb-eb55-c3504b2be20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total test dog images: 250\n"
          ]
        }
      ],
      "source": [
        "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "بنابراین، 2000 تصویر آموزشی، 1000 تصویر اعتبارسنجی و 1000 تصویر آزمایشی در اختیار داریم و تعداد نمونه‌های کلاس‌ها در هر بخش از تمام بخش‌ها برابر است؛ بنابراین این یک مسئله دسته‌بندی دودویی متوازن است و دقت دسته‌بندی یک معیار مناسب برای ارزیابی آن خواهد بود.\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "HOD7BRrZcv_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"right\">\n",
        "<font size=\"+2\" face=\"homa\">\n",
        "  <b>\n",
        "  ساخت شبکه\n",
        "  <br><br>\n",
        "  <font size=\"+1\" face=\"homa\">\n",
        "  </b>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "<br>\n",
        "در مثال قبلی یک شبکه کانولوشنی کوچک برای مجموعه داده MNIST ساختیم، بنابراین با چنین شبکه‌های کانولوشنی آشنا هستید. در این بخش از همان ساختار کلی مجدداً استفاده خواهیم کرد: شبکه کانولوشنی، پشته‌ای از لایه‌های یک در میانConv2D  (با فعال‌سازی relu) و MaxPooling2D خواهد بود؛ اما از آنجایی که در اینجا با تصاویر بزرگ‌تر و مسائل پیچیده‌تری سر و کار داریم، شبکه را نیز به همان نسبت بزرگ‌تر خواهیم کرد: این شبکه، یک مرحله Conv2D + MaxPooling2D بیشتر خواهد داشت. این مرحله باعث تقویت ظرفیت شبکه شده و از اندازه نقشه‌های ویژگی خواهد کاست تا هنگام رسیدن به لایه Flatten، نقشه‌ها بیش از اندازه بزرگ نباشند. در اینجا، چون با ورودی‌هایی با اندازه 150 × 150 شروع کرده‌ایم (انتخاب تقریباً تصادفی)، درست قبل از لایه Flatten با نقشه‌های ویژگی 7 × 7 کار را تمام خواهیم کرد.\n",
        "<br>\n",
        "نکته: عمق نقشه‌های ویژگی به طور تصاعدی در شبکه افزایش می‌یابد (از 32 به 128)، در حالی که اندازه آن‌ها کاهش می‌یابد (از 148 × 148 به 7 × 7). تقریباً در تمامی شبکه‌های کانولوشنی این الگو را خواهید دید.\n",
        "<br>\n",
        "به دلیل اینکه با مسئله دسته‌بندی دودویی مواجه هستیم، شبکه را با یک واحد (لایه متراکم با سایز 1) و فعال‌سازی سیگموید به پایان خواهیم برد. این واحد، احتمال تعلق یک نمونه به یکی از دو کلاس را مشخص خواهد کرد.\n",
        "<br>"
      ],
      "metadata": {
        "id": "QuTz054sdDGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0z0MfJpKLwEa"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "حال می‌خواهیم ببینیم ابعاد نقشه‌های ویژگی با هر لایه متوالی چه تغییری می‌کند:\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "X-DhInwldis8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5vuDh6GLwEc",
        "outputId": "d38215b2-e04d-4870-fbd5-28ca141c9d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 15, 15, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 7, 7, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               3211776   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,453,121\n",
            "Trainable params: 3,453,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "طبق معمول، برای مرحله کامپایل مدل از بهینه‌ساز RMSprop استفاده خواهیم کرد. از آنجایی که شبکه با یک واحد سیگموید به پایان می‌رسد، به عنوان خطا از آنتروپی متقابل دودویی استفاده خواهیم کرد \n",
        "<br><br>"
      ],
      "metadata": {
        "id": "DUJswLmhdrYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTQc_VeULwEc",
        "outputId": "79bb186c-2ebf-406f-ee6a-85b0272a36d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from keras import optimizers\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"right\">\n",
        "<font size=\"+2\" face=\"homa\">\n",
        "  <b>\n",
        "  پیش‌پردازش داده‌ها\n",
        "  <br><br>\n",
        "  <font size=\"+1\" face=\"homa\">\n",
        "  </b>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "<br>\n",
        "همانطور که می‌دانید، نمونه‌ها قبل از ورود به شبکه باید به طور متناسب به تنسور ممیز شناور تبدیل شوند. در حال حاضر، نمونه‌ها به عنوان فایل‌های JPEG روی دیسک هستند، بنابراین گام‌های رساندن آن‌ها به شبکه تقریباً به شکل زیر خواهد بود:\n",
        "1)\tفایل‌های تصاویر را بخوانید.\n",
        "<br>\n",
        "2)\tمحتوای تصاویر JPEG را به ماتریسی از پیکسل¬های RGB تبدیل کنید.\n",
        "<br>\n",
        "3)\tاین ماتریس‌ها را به تنسورهای ممیز شناور تبدیل کنید.\n",
        "<br>\n",
        "4)\tمقادیر پیکسل (بین 0 و 255) را به بازه [1، 0] تغییر دهید (همانطور که می‌دانید شبکه‌های عصبی مدیریت مقادیر ورودی کوچک را ترجیح می‌دهند).\n",
        "<br>\n",
        "ممکن است تا حدودی سخت به نظر برسد، اما خوشبختانه کراس توابعی دارد که به طور خودکار این گام‌ها را انجام می‌دهند. کراس یک ماژول تحت عنوان keras.preprocessing.image حاوی ابزارهای کمکی برای پردازش تصویر دارد. به ویژه، حاوی کلاس ImageDataGenerator است که امکان راه‌اندازی سریع مولدهای پایتون  را فراهم می‌کند؛ این مولدها می‌توانند به طور خودکار فایل‌های تصویر روی دیسک را به دسته‌های تنسورهای پیش‌پردازش شده تبدیل نمایند. در اینجا از این روش استفاده خواهیم کرد.\n",
        "<br>"
      ],
      "metadata": {
        "id": "vYDL3sydd24e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsbxpqp-LwEe",
        "outputId": "039b7866-655e-4e59-ae95-b547aefcd0b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 150x150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "حال می‌خواهیم خروجی یکی از این مولدها را بررسی کنیم: دسته‌های 150 × 150 از تصاویر RGB (با ابعاد (20, 150, 150, 3)) و برچسب‌های دودویی (با ابعاد (20, )) تولید می‌کند. در هر دسته 20 نمونه وجود دارد (اندازه دسته). توجه کنید که مولد، این دسته‌ها را به صورت نامحدود تولید می‌کند: به عبارت دیگر تصاویر پوشه هدف در یک حلقه تکرار بی‌پایان قرار می‌گیرند. به همین دلیل باید حلقه تکرار را در جایی بشکنید:\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "PQu4tVggeRyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waWGx9k9LwEf",
        "outputId": "9d3ecd71-bdb7-4e10-88bd-31efd76eb888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data batch shape: (20, 150, 150, 3)\n",
            "labels batch shape: (20,)\n"
          ]
        }
      ],
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "    print('data batch shape:', data_batch.shape)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "حال می‌خواهیم عمل fit را با نمونه‌های تولید شده به وسیله مولد اجرا کنیم. برای این کار باید از متد fit_generator استفاده کنیم که معادل fit برای مولدها است. اولین ورودی این متد مولدی با قابلیت تولید دسته‌هایی از ورودی‌ها و خروجی‌ها به طور نامحدود است. از آنجایی که نمونه‌ها به‌ صورت بی‌پایان تولید می‌شوند، مدل کراس باید بداند قبل از اعلام اتمام یک تکرار چه تعداد نمونه از مولد بگیرد. این نقش آرگومان steps_per_epoch است: بعد از گرفتن دسته‌هایی به میزان steps_per_epoch از مولدها (به عبارتی بعد از اجرای گرادیان نزولی به تعداد گا¬م¬هایی برابر با steps_per_epoch، فرایند برازش به تکرار بعدی خواهد رفت). در مثال فوق دسته‌ها 20 نمونه دارند، بنابراین مولد، 100 دسته را خواهد گرفت تا به هدف خود یعنی 2000 نمونه برسید.\n",
        "<br>\n",
        "هنگام استفاده از متد fit_generator، مانند متد fit می‌توانید یک آرگومان validation_data وارد کنید. توجه به این نکته حائز اهمیت است که این آرگومان می‌تواند مولد داده باشد، اما همچنین می‌تواند یک تاپل‌ از آرایه‌های نام‌پای باشد. در صورتی که مولدی را به عنوان validation_data وارد کنید، این مولد باید دسته‌های نمونه‌های اعتبارسنجی را به طور بی‌پایان تولید کند؛ بنابراین باید آرگومان validation_steps را نیز مشخص کنید که تعداد دسته‌هایی را تعیین می‌کند که فرایند باید از مولد اعتبارسنجی برای ارزیابی بگیرد.\n",
        "<br>"
      ],
      "metadata": {
        "id": "k9DFadsbeexL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "koSENsWLLwEg",
        "outputId": "657b95a0-ec24-48cc-a451-8fd2e00d464d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-a7acfc8093a4>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 81/100 [=======================>......] - ETA: 20s - loss: 0.4352 - acc: 0.8000"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-a7acfc8093a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit_generator(\n\u001b[0m\u001b[1;32m      2\u001b[0m       \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2258\u001b[0m         \u001b[0;34m'Please use `Model.fit`, which supports generators.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m         stacklevel=2)\n\u001b[0;32m-> 2260\u001b[0;31m     return self.fit(\n\u001b[0m\u001b[1;32m   2261\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "بهتر است همیشه بعد از آموزش، مدل‌ها را ذخیره کنید.\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "I9eRQ7mMexp-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "u0Vj60KvLwEh"
      },
      "outputs": [],
      "source": [
        "model.save('cats_and_dogs_small_1.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "حال می‌خواهیم نمودار خطا و دقت مدل را در طول آموزش روی نمونه‌های اعتبارسنجی و آموزش رسم کنیم \n",
        "<br><br>"
      ],
      "metadata": {
        "id": "-GnUMGkJe5cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "7LGCfmOoi8bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "cQLQ52wJLwEh",
        "outputId": "8562513b-e349-4a67-ac0a-46af286e050b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-ce9171dbf527>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_acc'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "این نمودارها نشان‌دهنده بیش‌برازش هستند. دقت آموزش در طول زمان به طور خطی افزایش یافته و به نزدیک 100 درصد می‌رسد، در حالی که دقت اعتبارسنجی در 72-70 درصد متوقف می‌شود. خطای اعتبارسنجی تنها بعد از پنج تکرار به حداقل رسیده و سپس بهبود آن متوقف می‌شود، در حالی که خطای آموزش تا رسیدن به صفر به کاهش خطی ادامه می‌دهد.\n",
        "<br>\n",
        "به دلیل داشتن نمونه‌های آموزشی اندک (2000)، بیش‌برازش، اولین دغدغه شما خواهد بود. برخی از راهکارها مانند حذف تصادفی و کنترل وزن (تنظیم L2) که قبلاً معرفی کردیم، می‌توانند برای تعدیل بیش‌برازش مفید واقع شوند. حال می‌خواهیم از راهکار جدیدی استفاده کنیم که مختص بینایی ماشین است و هنگام پردازش تصاویر با مدل‌های یادگیری عمیق تقریباً کاربرد فراگیر دارد. این راهکار داده¬افزایی است.\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "O78xJ3bVe_SF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"right\">\n",
        "<font size=\"+2\" face=\"homa\">\n",
        "  <b>\n",
        "استفاده از داده افزایی\n",
        "  <br><br>\n",
        "  <font size=\"+1\" face=\"homa\">\n",
        "  </b>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "<br>\n",
        "بیش‌برازش، نتیجه استفاده از تعداد نمونه‌های اندک برای یادگیری است و موجب ناتوانی در آموزش مدلی می‌شود که قابل تعمیم به نمونه‌های جدید باشد. با داشتن نمونه‌های نامحدود، مدل با هر جنبه احتمالی توزیع نمونه‌های موجود روبرو خواهد شد و هرگز بیش‌برازش روی نخواهد داد. داده¬افزایی رویکرد تولید نمونه‌های آموزشی بیشتر از روی نمونه‌های آموزشی موجود است. در این روش، نمونه‌ها از طریق تعدادی تبدیل‌ تصادفی افزایش می‌یابند و تصاویری شبه واقعی تولید می‌شود. هدف از این کار، دوری جستن از نمایش دوباره یک تصویر برای مدل در هنگام آموزش است. با این کار، مدل با جوانب بیشتری مواجه شده و بهتر تعمیم می‌یابد.\n",
        "<br>\n",
        "در کراس، برای رسیدن به این مقصود تعدادی از تبدیل‌های تصادفی به گونه‌ای پیکربندی می‌شوند که روی تصاویر خوانده شده با ImageDataGenerator اعمال شوند. با یک مثال کارمان را شروع می‌کنیم.\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "BfpqUVT2fG6O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MZnOPlM_LwEi"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "این‌ها فقط چند نمونه از گزینه‌های قابل دسترس هستند (برای نمونه‌های بیشتر به مستندات کراس مراجعه کنید). اکنون، نگاهی گذرا به تبدیل‌های مورد استفاده در کد خواهیم داشت:\n",
        "<br>\n",
        "*\trotation_range مقداری در بازه صفر تا 180 درجه است، دامنه‌ای که تصاویر در آن به طور تصادفی می‌چرخند.\n",
        "<br>\n",
        "*\twidth_shift و height_shift دامنه‌هایی هستند (به عنوان کسری از عرض یا ارتفاع کل) که در آن‌ها تصاویر به طور تصادفی به صورت عمودی یا افقی جابه‌جا می‌شوند.\n",
        "<br>\n",
        "*\tshear_range برای اعمال تصادفی تبدیل‌های برشی  به کار می‌رود.\n",
        "<br>\n",
        "*\tzoom_range برای بزرگ‌نمایی تصادفی در داخل تصاویر به کار می‌رود.\n",
        "<br>\n",
        "*\thorizontal_flip برای برگرداندن تصادفی نصف تصاویر به صورت افقی به کار می‌رود. زمانی کاربرد دارد که هیچ فرضی برای عدم تقارن افقی وجود ندارد (به عنوان مثال، تصاویر دنیای واقعی).\n",
        "<br>\n",
        "*\tfill_mode راهبردی برای پر کردن پیکسل‌هایی است که جدیداً به وجود آمده¬اند. علت ظهور این پیکسل‌ها چرخش یا جابه‌جایی عرض یا ارتفاع است.\n",
        "<br>\n",
        "حال می‌خواهیم تصاویر افزوده را بررسی کنیم \n",
        "<br>"
      ],
      "metadata": {
        "id": "nNPpNmgXfegd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVHu1oQ0LwEj"
      },
      "outputs": [],
      "source": [
        "# This is module with image preprocessing utilities\n",
        "from keras.preprocessing import image\n",
        "\n",
        "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
        "\n",
        "# We pick one image to \"augment\"\n",
        "img_path = fnames[3]\n",
        "\n",
        "# Read the image and resize it\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "\n",
        "# Convert it to a Numpy array with shape (150, 150, 3)\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "# Reshape it to (1, 150, 150, 3)\n",
        "x = x.reshape((1,) + x.shape)\n",
        "\n",
        "# The .flow() command below generates batches of randomly transformed images.\n",
        "# It will loop indefinitely, so we need to `break` the loop at some point!\n",
        "i = 0\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "    plt.figure(i)\n",
        "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
        "    i += 1\n",
        "    if i % 4 == 0:\n",
        "        break\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "اگر شبکه جدیدی را با استفاده از این پیکربندی داده¬افزایی آموزش دهید، شبکه هرگز یک ورودی را دو بار نخواهد دید. با این وجود ورودی‌هایی که می‌بیند به شدت همبستگی دارند، چرا که همگی از چند تصویر اصلی مشتق شده‌اند. پس نمی‌توانید اطلاعات جدیدی تولید کنید. تنها کاری که می‌توانید بکنید ترکیب مجدد اطلاعات موجود است. در نتیجه شاید این راهکار برای خلاص شدن کامل از بیش‌برازش کافی نباشد. برای مقابله بیشتر با بیش‌برازش، درست قبل از دسته‌بند تمام متصل، یک لایه حذف تصادفی به مدل اضافه خواهیم کرد.\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "j8a3UYUYf8oP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Vi9j02hnLwEk"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "حال می‌خواهیم شبکه را با داده‌افزایی و حذف تصادفی آموزش دهیم.\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "CazURKIPgHfP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8W-YQYfLwEk"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,)\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 150x150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "مدل را ذخیره می‌کنیم و اکنون مجدداً برای نتایج، نمودار رسم می‌کنیم\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "knDuuHtFgR_d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XmWJQ3ugLwEl"
      },
      "outputs": [],
      "source": [
        "model.save('cats_and_dogs_small_2.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "اکنون مجدداً برای نتایج، نمودار رسم می‌کنیم\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "5jn0t0P6gfB8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1QAbjiILwEm"
      },
      "outputs": [],
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" align=\"justify\">\n",
        "<font face=\"homa\">\n",
        "در نتیجه داده‌افزایی و حذف تصادفی از بیش‌برازش خبری نیست: منحنی‌های آموزش، نزدیک به منحنی‌های اعتبارسنجی قرار گرفته‌اند. حالا، به دقت 82 درصد رسیده‌ایم، یعنی بهبود 15 درصدی نسبت به مدل تنظیم نشده.\n",
        "<br>\n",
        "استفاده بیشتر از فنون تنظیم وزن (همچون L2) و تنظیم پارامترهای شبکه (مانند تعداد فیلترها در هر لایه کانولوشن، یا تعداد لایه‌ها در شبکه) می‌تواند به کسب دقت بیشتر تا 86 درصد یا 87 درصد کمک نماید؛ اما به ‌دلیل استفاده از نمونه‌های بسیار کم، بالا بردن این درصد، با آموزش از صفر شبکه کانولوشنی دشوار خواهد بود. برای بهبود دقت در این مسئله، به عنوان گام بعدی از مدل از قبل آموزش  دیده استفاده خواهیم کرد که در دو بخش بعدی توضیح داده شده است.  \n",
        "<br>"
      ],
      "metadata": {
        "id": "ysTUHFFpgna1"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}